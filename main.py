#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import IntegerType, DoubleType
import matplotlib.pyplot as plt

if __name__ == "__main__":
    # Creating Spark Session
    spark = SparkSession.builder.appName('SalesDataAnalysis').getOrCreate()
    
    print("""Data Loading: Load the 'sales_data.csv' dataset into a Spark DataFrame.""")
    
    # CSV has a header row, so header=True
    df = spark.read.csv('sales_data.csv', header=True)
    
    
    # transaction_id (integer): A unique identifier for each transaction.
    # customer_id (integer): The ID of the customer making the purchase.
    # product_id (integer): The ID of the product being purchased.
    # timestamp (timestamp): The timestamp of the transaction.
    # quantity (integer): The quantity of the product sold in this transaction.
    # price (double): The price of one unit of the product.
    # total_amount (double): The total amount for this transaction (quantity * price).
    
    df = df.withColumn('transaction_id', df.transaction_id.cast(IntegerType()))
    df = df.withColumn('customer_id', df.customer_id.cast(IntegerType()))
    df = df.withColumn('product_id', df.product_id.cast(IntegerType()))
    df = df.withColumn('timestamp', to_timestamp(df.timestamp, 'yyyy-MM-dd'))
    df = df.withColumn('quantity', df.quantity.cast(IntegerType()))
    df = df.withColumn('price', df.price.cast(DoubleType()))
    df = df.withColumn('total_amount', df.total_amount.cast(DoubleType()))
    
    df.printSchema()
    
    print("""Data Exploration: Provide summary statistics for the dataset, including the count, mean, standard deviation, minimum, and maximum values for each numeric column.""")
    print('BEFORE data cleaning:')
    df.describe().show()
    
    print("""
          To be noted:
              Some unrealistic results such as negative minimum quantity and negative minimum price showing the presence of erroneous data in the dataset.
          
            """)
    
    print("""Data Cleaning: Check for and handle any missing or erroneous data in the dataset. Document the steps you take to clean the data.""")
    
    # Check initial count of rows
    print('Initial Count: ', df.count())
    print('')
    print('Check for missing data (beginning of the process):')
    na_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()
    
    # Drop rows with any null values
    df = df.na.drop(how='any')
    
    # Check count after removing null values
    print('Count after dropping nulls: ', df.count())
    
    # Check for erroneous data: Only keeping data that satisfy the following criterias:
    # # 1. Where the quantity is positive
    # # 2. Where total amount is equal to quantity * price
    # # 3. Where timestamp is plausible (not in the future nor too far in the past)
    df = df.filter(df.quantity >= 0)
    df = df.filter((df.quantity * df.price) == df.total_amount)
    df = df.filter((df.timestamp >= '2023-01-01') & (df.timestamp <= '2023-12-31'))
    
    # Check count after removing erroneous rows
    print('Count after dropping erroneous rows: ', df.count())
    print('')
    print('Check for missing data (end of the process):')
    na_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()
    
    print('AFTER data cleaning:')
    df.describe().show()
    print("""
          To be noted:
              Realistic minimum quantity and realistic minimum price.
              The cleaned dataset delivers plausible exploratory analysis.
          """)
    
    print("""Data Transformation: Perform the following transformations:""")
    print("""a. Calculate the total revenue generated by each product (sum of total_amount for each product).""")
    
    revenue_by_product = df.groupBy('product_id').agg(round(sum('total_amount'), 2).alias('total_revenue')).orderBy(desc('total_revenue'))
    revenue_by_product.show(100)
    #note: Rounded for better readability.
    
    print("""b. Determine the top 10 selling products based on the total quantity sold.""")
    
    top_10_selling_products = df.groupBy('product_id').sum('quantity').orderBy(desc('sum(quantity)')).limit(10)
    top_10_selling_products.show()

    spark.stop()
